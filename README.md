# 2022.5.20
## 仓库总体介绍
#### 前期主要研究最新NAS方向的一些工作，后期投完简历时开始研究深度学习算法的FPGA和ASIC专用芯片硬件加速实现。

# 2022.5.21
#### 重新阅读DARTS ，完成阅读报告
#### 1.DARTS其实是一个搜索加上训练的模型，即先在一个数据集上搜索收敛得到模型参数和架构参数，在重新跑一个数据集得到准确率，但是搜出来的模型为什么适用不同数据集呢
#### 2.DARTS的batch_size不能调的太大，显存会爆，也是论文说的是用cifar10训练的原因。而不是imagenet。
#### 3.DARTS是由cell堆叠起来的，每一层一个cell，但是cell（两类）是一模一样的，这样感觉没能完全自动化搜索。
#### 4.为什么搜索时采用的标准来决定的操作一定在重新训练时也是最优的。
#### 5.这个连续松弛为什么softmax后选出的一个候选操作将其变为1会大于几个候选操作分别各自乘以权？

# 2022.5.23
#### 阅读了PC-DARTS，完成报告
#### 1.减少信道数我觉得是和batchsize减少和dropout差不多的思路，所以并没有是一个大的改变，因为你整个网络喂入的数据变少了。
#### 2.edge normalization是在两两节点之间加上参数而不是在两个节点之内加上参数。这个还是蛮巧妙的。
#### 3.文章的代码量感觉会非常少。

# 2022.5.24
#### 阅读了Big NAS，完成报告
#### 1.BigNAS是直接训练的一个大的超网，里面会有几千个小模型（搜索空间的堆叠，里面其实是一个权重共享或者说是蒸馏的方式），最终参数优化到一个比较好的地方，然后遇到具体的硬件限制我再从里面具体搜索。
#### 2.这个权重共享是否就是DARTS中的连续松弛和优化？那么创新的点不就是使用策略将所有候选模块都变得比较好而且能够留到搜索阶段？

#### 阅读了AttentiveNAS
#### 1.正式整理出两阶段的概念，即先进行训练（这部分也得确定合适的架构参数，这部分在DARTS中一直说的都是搜索，所以之前一直很难理解这个概念）确定出好的网络结构，然后再进行挑选，不能说是搜索，得到自己任务下想要的结果。
#### 2.说是将搜索和训练联系到一起，其实就是有返回darts的意思，将硬件的正则加入搜索的过程之中。
#### 3.但是这样以来他的搜索阶段在哪里？这不是直接要了一个最好的模型？

# 2022.5.25
####  阅读了B-DARTS
#### 1.引入架构参数正则的数学推导还可以。
#### 2。代码量太少，还是基于最初的DARTS做的一些工作。




# 总结和展望
#### 1.可以在DARTS改进方向和two-stage思路上两个角度做进一步优化，是否能够提出更好的搜索策略，来实现计算量和准确率的优化，这部分读到的论文很多。
#### 2.关于搜索空间，考虑更大更复杂的搜索空间，卷积核的长，宽，步长，参数，层数，信道数……会怎么样，或者能不能实现将卷积，循环融合到一个搜索空间中去，能不能将transformer加进去，这部分读到的论文很少。
#### 3.神经结构搜索算法在更多任务中评估表现，因为NAS的很多论文都是在imagenet和cifar数据集上做的，都是图像类的，不同的数据集例如文本？是否会有不同的表现。这部分读到的论文也很少
#### 4.架构参数和模型参数的不平衡，如何更好地更新参数问题。
#### 5.在现有的NAS方法上进行通用的加速研究，NAS搜索需要的时间和计算资源过大。

