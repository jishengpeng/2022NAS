# 2022.5.20
## 仓库总体介绍
#### 主要研究最新NAS方向的一些工作

# 2022.5.21
#### 重新阅读DARTS ，完成阅读报告
#### 1.DARTS其实是一个搜索加上训练的模型，即先在一个数据集上搜索收敛得到模型参数和架构参数，在重新跑一个数据集得到准确率。
#### 2.DARTS的batch_size不能调的太大，显存会爆，也是论文说的是用cifar10训练的原因。而不是imagenet。
#### 3.DARTS是由cell堆叠起来的，每一层一个cell，但是cell（两类）是一模一样的，这样感觉没能完全自动化搜索。确实cell的结构太固定了
#### 4.为什么搜索时采用的标准来决定的操作一定在重新训练时也是最优的。
#### 5.这个连续松弛为什么softmax后选出的一个候选操作将其变为1会大于几个候选操作分别各自乘以权？

# 2022.5.23
#### 阅读了PC-DARTS，完成报告
#### 1.减少信道数我觉得是和batchsize减少和dropout差不多的思路，所以并没有是一个大的改变，因为你整个网络喂入的数据变少了。
#### 2.edge normalization是在两两节点之间加上参数而不是在两个节点之内加上参数。这个还是蛮巧妙的。
#### 3.文章的代码量感觉会非常少。

# 2022.5.24
#### 阅读了Big NAS，完成报告
#### 1.BigNAS是直接训练的一个大的超网，里面会有几千个小模型（搜索空间的堆叠，里面其实是一个权重共享或者说是蒸馏的方式），最终参数优化到一个比较好的地方，然后遇到具体的硬件限制我再从里面具体搜索。
#### 2.这个权重共享是否就是DARTS中的连续松弛和优化？那么创新的点不就是使用策略将所有候选模块都变得比较好而且能够留到搜索阶段？

#### 阅读了AttentiveNAS
#### 1.正式整理出两阶段的概念，即先进行训练（这部分也得确定合适的架构参数，这部分在DARTS中一直说的都是搜索，所以之前一直很难理解这个概念）确定出好的网络结构，然后再进行挑选，不能说是搜索，得到自己任务下想要的结果。
#### 2.说是将搜索和训练联系到一起，其实就是有返回darts的意思，将硬件的正则加入搜索的过程之中。
#### 3.但是这样以来他的搜索阶段在哪里？这不是直接要了一个最好的模型？

# 2022.5.25
####  阅读了B-DARTS
#### 1.引入架构参数正则的数学推导还可以。
#### 2。代码量太少，还是基于最初的DARTS做的一些工作。


# 2022.5.26 总结一
#### 1.可以在DARTS改进方向和two-stage思路上两个角度做进一步优化，是否能够提出更好的搜索策略，来实现计算量和准确率的优化，这部分读到的论文很多。
#### 2.关于搜索空间，考虑更大更复杂的搜索空间，卷积核的长，宽，步长，参数，层数，信道数……会怎么样，或者能不能实现将卷积，循环融合到一个搜索空间中去，能不能将transformer加进去，这部分读到的论文很少。
#### 3.神经结构搜索算法在更多任务中评估表现，因为NAS的很多论文都是在imagenet和cifar数据集上做的，都是图像类的，不同的数据集例如文本？是否会有不同的表现。这部分读到的论文也很少
#### 4.架构参数和模型参数的不平衡，如何更好地更新参数问题。
#### 5.在现有的NAS方法上进行通用的加速研究，NAS搜索需要的时间和计算资源过大。

# 2022.6.24 新增DARTS代码解析版
#### 1.针对的是DARTS的CNN部分代码，具体的解析写在了代码的注释中，核心地方的代码（两百行）几乎是每一行一句注释，这个地方总体进行概述（读代码之前需要对DARTS论文比较理解）
#### 2.1 architect.py这部分涉及了架构参数更新，其中用到了论文中经过数学推导用到的梯度近似的方法（这部分code可以跳过不看）
#### 2.2 genotype.py这部分是辅助型文件，PRIMITIVES宏观上定义了每两个节点之间的所有的八种操作。然后Genotype这里面其实是已经确定好的操作，darts中一个cell有四个node，每两个node之间最终确定一条operation，但是每个mode的输入会从比他序号小的node们中选两个cat起来，所以加起来一个cell会有八个operation。
#### 2.3 model_search.py这部分是很关键的部分，其中定义了模型是由cell怎么构成的，cell是怎么由node和opeartion更新的，模型的架构参数的初始化等等，这部分我注解写的非常详细
#### 2.4 opeartion.py这部分是辅助性文件，详细定义了八种operation部分，可以看出来，其实就是一些卷积层和激活层，以及常用层的堆叠，但是这些堆叠的可解释性还是值得研究。然后还定义了一些其他的神经网络聚合层。
#### 2.5 train_search.py这部分是关键文件，是程序的入口，这部分进行了模型的搜索，开头一堆超参数的定义，日志啥的得看懂，后面就是数据的加载，预处理，训练集测试集。神经网络训练的过程，得到准确率等等（这部分调用了很多关键文件和非关键文件例如architect和model_search）
#### 2.6 utils.py这部分是辅助文件，定义了一些函数被其他部分调用，例如数据预处理函数，保存函数等等
#### 2.7 visualize.py这部分画图用，画出cell的内部结构图，用到的是genotype。
#### 2.8 model.py这部分是根据前面搜索出来的模型，重头搭建一个真正的模型，大部分继承了model_serach的逻辑。
#### 2.9 train.py和test.py   train.py就是在模型上重新从头训练，让准确率更好看。用到的是NetworkCIFAR作为network(model)。test是在测试集上训练（其实感觉没必要，因为train里面有训练集和测试集）
#### 2.10 trtain_imagenet.py和test_imagenet.py 重新在另外一个数据集中跑结果，network（在model.py中）是不一样的。

# 2022.6.24 DARTS代码问与答
#### 1.架构参数，模型参数是怎么更新的。
#### 答：论文中说在测试集上更新架构参数，在训练集上更新模型参数（其实是很假的有误导倾向），其实DARTS分为两部分搜索阶段和重新训练阶段！搜索阶段用到cifar的训练集，他将训练集分成两部分，一部分叫训练部分，一部分叫测试部分（其实都是训练集分出来的），然后train_search过程中在一个epoch中就更新了架构参数和训练参数
![image](https://user-images.githubusercontent.com/78149477/175483242-c3138611-b1df-4f89-863c-6e2d5ada4afe.png)
然后搜索阶段还用训练集的验证部分来测试准确率（这时完全不更新参数）。在搜索结束后的重新训练阶段！又有训练集重新模型参数和测试集验证准确率（重新训练阶段跑一次训练集更新模型参数，跑两次验证集得准确率）！
#### 2.每个node（其实就是1，2，3，4）怎么形成一个cell
#### 答：单纯的cat堆叠
#### 3.注意区分：每个cell入口有两个输入，分别是（n-1）cell和（n-2）cell。每个node最终会在此node之前选两个node的特征图cat起来作为本node的输入，任意两个node之间会有八个候选operation，最后只会选一个operation
#### 4.conv咋还分dill sep的
#### 答：分别是空洞卷积和深度可分离卷积
![image](https://user-images.githubusercontent.com/78149477/175484855-ae65bd45-317b-49bb-82b4-22c6c886e2ea.png)
上面是空洞卷积，下面是深度可分离卷积![image](https://user-images.githubusercontent.com/78149477/175485581-bc1cdd94-581b-44c0-8a83-be9f138984b2.png)




