# 2022.5.20
## 仓库总体介绍
#### 想跟着北深的老师做些研究，所以用这个仓库来记录研究进度。
#### 前期主要研究最新NAS方向的一些工作，后期投完简历时开始研究深度学习算法的FPGA和ASIC专用芯片硬件加速实践。

# 2022.5.11
#### 重新阅读DARTS并完成阅读报告，说点论文中没有提及但是代码中的问题
#### 1.DARTS其实是一个搜索加上训练的模型，即先在一个数据集上搜索收敛得到模型参数和架构参数，在重新跑一个数据集得到准确率，这样你搜出来的模型为什么适用所有数据集呢
#### 2.DARTS的batch_size不能调的太大，显存会爆，这就是论文说的是用cifar10训练的原因。而不是imagenet。
#### 3.DARTS是由cell堆叠起来的，每一层一个cell，但是cell（两类）是一模一样的，为什么要这样设计网络结构。

# 2022.5.13
#### 阅读了PC-DARTS完成报告，说点想法
#### 1.减少信道数我觉得本质上和batchsize减少和dropout差不多的思路，所以并没有是一个大的改变，因为你整个网络喂入的数据变少了。
#### 2.edge normalization是在两两节点之间加上参数而不是在两个节点之内加上参数。这个还是蛮巧妙的。
#### 3.文章的代码量感觉会非常少。
